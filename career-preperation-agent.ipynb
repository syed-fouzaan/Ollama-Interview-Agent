{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9983c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autogen-agentchat~=0.2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (0.7.2)\n",
      "Requirement already satisfied: autogen-core==0.7.2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-agentchat~=0.2) (0.7.2)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat~=0.2) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.34.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat~=0.2) (1.36.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat~=0.2) (11.2.1)\n",
      "Requirement already satisfied: protobuf~=5.29.3 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat~=0.2) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat~=0.2) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat~=0.2) (4.14.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.34.1->autogen-core==0.7.2->autogen-agentchat~=0.2) (8.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.2->autogen-agentchat~=0.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.2->autogen-agentchat~=0.2) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.2->autogen-agentchat~=0.2) (0.4.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.7.2->autogen-agentchat~=0.2) (3.23.0)\n",
      "Requirement already satisfied: chromadb in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (1.0.16)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (1.72.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from chromadb) (4.25.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.32.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from sentence_transformers) (2.6.0+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.32.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from sentence_transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.8.3)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
      "Requirement already satisfied: pypdf in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (5.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"autogen-agentchat~=0.2\"\n",
    "!pip install chromadb\n",
    "!pip install sentence_transformers\n",
    "!pip install tiktoken\n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c68904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogen[all]\n",
      "  Downloading pyautogen-0.10.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: autogen-agentchat>=0.6.4 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pyautogen[all]) (0.7.2)\n",
      "Requirement already satisfied: autogen-core==0.7.2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-agentchat>=0.6.4->pyautogen[all]) (0.7.2)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.34.1 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (1.36.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (11.2.1)\n",
      "Requirement already satisfied: protobuf~=5.29.3 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (4.14.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.34.1->autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (8.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (0.4.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\syed fouzan\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.7.2->autogen-agentchat>=0.6.4->pyautogen[all]) (3.23.0)\n",
      "Downloading pyautogen-0.10.0-py3-none-any.whl (3.0 kB)\n",
      "Installing collected packages: pyautogen\n",
      "Successfully installed pyautogen-0.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pyautogen 0.10.0 does not provide the extra 'all'\n"
     ]
    }
   ],
   "source": [
    "pip install pyautogen[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643f47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbef3329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syed fouzan\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\Users\\syed fouzan\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "c:\\Users\\syed fouzan\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "import autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82fb53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "from autogen import AssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "from autogen.agentchat.user_proxy_agent import UserProxyAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b513b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function that simulates some asynchronous task (e.g., I/O operation)\n",
    "\n",
    "\n",
    "async def my_asynchronous_function():\n",
    "    print(\"Start asynchronous function\")\n",
    "    await asyncio.sleep(2)  # Simulate some asynchronous task (e.g., I/O operation)\n",
    "    print(\"End asynchronous function\")\n",
    "    return \"input\"\n",
    "\n",
    "\n",
    "# Define a custom class `CustomisedUserProxyAgent` that extends `UserProxyAgent`\n",
    "\n",
    "\n",
    "class CustomisedUserProxyAgent(UserProxyAgent):\n",
    "    # Asynchronous function to get human input\n",
    "    async def a_get_human_input(self, prompt: str) -> str:\n",
    "        # Call the asynchronous function to get user input asynchronously\n",
    "        user_input = await my_asynchronous_function()\n",
    "\n",
    "        return user_input\n",
    "\n",
    "    # Asynchronous function to receive a message\n",
    "\n",
    "    async def a_receive(\n",
    "        self,\n",
    "        message: Union[Dict, str],\n",
    "        sender,\n",
    "        request_reply: Optional[bool] = None,\n",
    "        silent: Optional[bool] = False,\n",
    "    ):\n",
    "        # Call the superclass method to handle message reception asynchronously\n",
    "        await super().a_receive(message, sender, request_reply, silent)\n",
    "\n",
    "\n",
    "class CustomisedAssistantAgent(AssistantAgent):\n",
    "    # Asynchronous function to get human input\n",
    "    async def a_get_human_input(self, prompt: str) -> str:\n",
    "        # Call the asynchronous function to get user input asynchronously\n",
    "        user_input = await my_asynchronous_function()\n",
    "\n",
    "        return user_input\n",
    "\n",
    "    # Asynchronous function to receive a message\n",
    "    async def a_receive(\n",
    "        self,\n",
    "        message: Union[Dict, str],\n",
    "        sender,\n",
    "        request_reply: Optional[bool] = None,\n",
    "        silent: Optional[bool] = False,\n",
    "    ):\n",
    "        # Call the superclass method to handle message reception asynchronously\n",
    "        await super().a_receive(message, sender, request_reply, silent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb1029b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_config(model, temperature, seed):\n",
    "    \"\"\"\n",
    "    Creates a configuration for a local model using Ollama.\n",
    "    \"\"\"\n",
    "    config_list = [\n",
    "        {\n",
    "            \"model\": model,  # The name of the model you downloaded, e.g., \"llama3\"\n",
    "            \"base_url\": \"http://localhost:11434/v1\",  # The local Ollama API endpoint\n",
    "            \"api_key\": \"ollama\",  # A placeholder is required, \"ollama\" works\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    llm_config = {\n",
    "        \"seed\": int(seed),\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": float(temperature),\n",
    "    }\n",
    "\n",
    "    return llm_config\n",
    "\n",
    "# --- How to use the function ---\n",
    "# Now you can create your llm_config by calling the function with the model you downloaded.\n",
    "# For example, if you downloaded 'llama3':\n",
    "\n",
    "# my_llm_config = create_llm_config(model=\"llama3\", temperature=0.7, seed=42)\n",
    "# print(my_llm_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cec53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "Resume Review, Technical Skills Assessment, Project Discussion, Job Role Expectations, Closing Remarks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 20:56:50] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "Let's create some interview questions for each agenda point.\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. Can you walk me through your background and how it relates to this role?\n",
      "2. What do you think are your greatest strengths and how have they been applied in previous positions?\n",
      "3. How do you handle [insert specific skill or scenario relevant to the job]?\n",
      "\n",
      "Please provide feedback for the Resume Review section:\n",
      "\n",
      "(Note: Please respond with any suggestions, changes, or if you're satisfied with the questions)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 20:57:49] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "It seems like you'd like me to generate new questions for the \"Resume Review\" section based on your input. However, I didn't receive any specific feedback or suggestions.\n",
      "\n",
      "To move forward, I can either:\n",
      "\n",
      "1. Generate new questions for the \"Resume Review\" section without incorporating your input.\n",
      "2. Terminate the process and exit since you didn't provide any feedback.\n",
      "\n",
      "Please let me know which option you prefer:\n",
      "\n",
      "(Type 'new' to generate new questions or 'exit' to terminate the process)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 20:58:58] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I'll generate new questions for the \"Resume Review\" section without incorporating your input. Here they are:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "Please provide feedback for these new questions:\n",
      "\n",
      "(Note: Please respond with any suggestions, changes, or if you're satisfied with the questions)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:00:10] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "It seems like you'd like me to generate new questions based on your input. However, I didn't receive any specific feedback or suggestions.\n",
      "\n",
      "To move forward, I can either:\n",
      "\n",
      "1. Generate new questions for the \"Resume Review\" section without incorporating your input.\n",
      "2. Terminate the process and exit since you didn't provide any feedback.\n",
      "\n",
      "Please let me know which option you prefer:\n",
      "\n",
      "(Type 'new' to generate new questions or 'exit' to terminate the process)\n",
      "\n",
      "If you want to exit, I'll stop generating questions and we can move on to the next agenda point.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:02:00] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I think there's been a misunderstanding! Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "So, let's move on to the next agenda point: **Technical Skills Assessment**!\n",
      "\n",
      "Here are some potential questions:\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "Please provide feedback for these questions:\n",
      "\n",
      "(Note: Please respond with any suggestions, changes, or if you're satisfied with the questions)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:03:17] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "You'd like me to generate new questions for the \"Technical Skills Assessment\" section based on your input. However, I didn't receive any specific feedback or suggestions.\n",
      "\n",
      "To move forward, I can either:\n",
      "\n",
      "1. Generate new questions for the \"Technical Skills Assessment\" section without incorporating your input.\n",
      "2. Terminate the process and exit since you didn't provide any feedback.\n",
      "\n",
      "Please let me know which option you prefer:\n",
      "\n",
      "(Type 'new' to generate new questions or 'exit' to terminate the process)\n",
      "\n",
      "If you want to exit, I'll stop generating questions and we can move on to the next agenda point.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:05:12] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I think there's been a misunderstanding! Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "So, let's move on to the next agenda point: **Project Discussion**!\n",
      "\n",
      "Here are some potential questions:\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "Please provide feedback for these questions:\n",
      "\n",
      "(Note: Please respond with any suggestions, changes, or if you're satisfied with the questions)\n",
      "\n",
      "If you don't want to update anything, we can move on to the next agenda point.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:07:08] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I think there's been a misunderstanding! Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "So, let's move on to the next agenda point: **Job Role Expectations**!\n",
      "\n",
      "Here are some potential questions:\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "Please provide feedback for these questions:\n",
      "\n",
      "(Note: Please respond with any suggestions, changes, or if you're satisfied with the questions)\n",
      "\n",
      "If you don't want to update anything, we can move on to the next agenda point.\n",
      "\n",
      "Type 'new' to generate new questions, or type 'exit' to terminate the process.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:08:49] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I think there's been a misunderstanding! Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "So, let's move on to the final agenda point: **Closing Remarks**!\n",
      "\n",
      "Here are some potential questions:\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Please provide feedback for these questions:\n",
      "\n",
      "(Note: Please respond with any suggestions, changes, or if you're satisfied with the questions)\n",
      "\n",
      "If you don't want to update anything, we've reached the end of the interview process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:13:56] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I think there's been a misunderstanding! Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "And that concludes our interview question generation process!\n",
      "\n",
      "Let me summarize the questions we've generated:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Thank you for participating in this process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:18:56] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I apologize, but it seems like we've reached a dead end!\n",
      "\n",
      "Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "And that concludes our interview question generation process!\n",
      "\n",
      "Let me summarize the questions we've generated:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Thank you for participating in this process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:22:49] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I apologize, but it seems like we've reached a dead end!\n",
      "\n",
      "Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "And that concludes our interview question generation process!\n",
      "\n",
      "Let me summarize the questions we've generated:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Thank you for participating in this process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:26:43] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I apologize, but it seems like we've reached a dead end!\n",
      "\n",
      "Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "And that concludes our interview question generation process!\n",
      "\n",
      "Let me summarize the questions we've generated:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Thank you for participating in this process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:30:53] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I apologize, but it seems like we've reached a dead end!\n",
      "\n",
      "Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "And that concludes our interview question generation process!\n",
      "\n",
      "Let me summarize the questions we've generated:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Thank you for participating in this process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:35:39] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I apologize, but it seems like we've reached a dead end!\n",
      "\n",
      "Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "And that concludes our interview question generation process!\n",
      "\n",
      "Let me summarize the questions we've generated:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Thank you for participating in this process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:40:01] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I apologize, but it seems like we've reached a dead end!\n",
      "\n",
      "Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "And that concludes our interview question generation process!\n",
      "\n",
      "Let me summarize the questions we've generated:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Thank you for participating in this process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:44:25] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I apologize, but it seems like we've reached a dead end!\n",
      "\n",
      "Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "And that concludes our interview question generation process!\n",
      "\n",
      "Let me summarize the questions we've generated:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Thank you for participating in this process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:48:50] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I apologize, but it seems like we've reached a dead end!\n",
      "\n",
      "Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "And that concludes our interview question generation process!\n",
      "\n",
      "Let me summarize the questions we've generated:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Thank you for participating in this process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-09 21:53:12] {714} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to boss):\n",
      "\n",
      "I apologize, but it seems like we've reached a dead end!\n",
      "\n",
      "Since you didn't provide any feedback or suggestions for the previous set of questions, I'm going to assume that you're satisfied with them.\n",
      "\n",
      "And that concludes our interview question generation process!\n",
      "\n",
      "Let me summarize the questions we've generated:\n",
      "\n",
      "**Resume Review**\n",
      "\n",
      "1. What motivated you to apply for this role, and how do you think your skills align with its requirements?\n",
      "2. Can you describe a project or initiative where you demonstrated leadership and took ownership of the outcome?\n",
      "3. How do you prioritize tasks and manage your time when working on multiple projects simultaneously?\n",
      "\n",
      "**Technical Skills Assessment**\n",
      "\n",
      "1. Can you explain how you would approach [specific technical challenge relevant to the job]?\n",
      "2. How do you stay current with industry developments and advancements in [specific technical area]?\n",
      "3. Describe a time when you had to troubleshoot a complex technical issue. What steps did you take, and what was the outcome?\n",
      "\n",
      "**Project Discussion**\n",
      "\n",
      "1. Can you walk me through a project you led from start to finish? What were your responsibilities, and what was the outcome?\n",
      "2. How do you prioritize tasks and manage multiple projects simultaneously?\n",
      "3. Describe a time when you had to collaborate with a cross-functional team to achieve a shared goal.\n",
      "\n",
      "**Job Role Expectations**\n",
      "\n",
      "1. What do you think are the most important skills and qualifications for someone in this role?\n",
      "2. How do you stay organized and manage competing priorities when working on multiple projects?\n",
      "3. Can you describe a time when you had to adapt to a changing project scope or unexpected challenges?\n",
      "\n",
      "**Closing Remarks**\n",
      "\n",
      "1. Is there anything else you'd like to add or discuss about your qualifications and fit for this role?\n",
      "2. What do you think sets you apart from other candidates, and why do you believe you're the best fit for this position?\n",
      "3. Are there any questions or concerns you have about the company or the role that I can address?\n",
      "\n",
      "Thank you for participating in this process!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start asynchronous function\n",
      "End asynchronous function\n",
      "\u001b[33mboss\u001b[0m (to assistant):\n",
      "\n",
      "input\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    boss = CustomisedUserProxyAgent(\n",
    "        name=\"boss\",\n",
    "        human_input_mode=\"ALWAYS\",\n",
    "        max_consecutive_auto_reply=0,\n",
    "        code_execution_config=False,\n",
    "    )\n",
    "\n",
    "    assistant = CustomisedAssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        system_message=\"You will provide some agenda, and I will create questions for an interview meeting. Every time when you generate question then you have to ask user for feedback and if user provides the feedback then you have to incorporate that feedback and generate new set of questions and if user don't want to update then terminate the process and exit\",\n",
    "        llm_config=create_llm_config(\"llama3\", \"0.4\", \"23\"),\n",
    "    )\n",
    "\n",
    "    await boss.a_initiate_chat(\n",
    "        assistant,\n",
    "        message=\"Resume Review, Technical Skills Assessment, Project Discussion, Job Role Expectations, Closing Remarks.\",\n",
    "        n_results=3,\n",
    "    )\n",
    "\n",
    "\n",
    "await main()  # noqa: F704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd5d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762531c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
